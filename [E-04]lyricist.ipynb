{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4c4f32",
   "metadata": {},
   "source": [
    "# 인공지능 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a7cb4",
   "metadata": {},
   "source": [
    "#### Step 1. 데이터 준비\n",
    "- 데이터 파일 읽기\n",
    "- 문장 단위로 저장\n",
    "\n",
    "#### Step 2. 데이터 전처리\n",
    "- 작사가를 만들기 위한 데이터 정제\n",
    "- 토큰화\n",
    "- 평가 데이터셋 분리\n",
    "- dataset 객체 생성\n",
    "\n",
    "#### Step 3. 인공지능 만들기\n",
    "- 모델 생성\n",
    "- 훈련 (val_loss 2.2 이하)\n",
    "- 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84582db",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf7e3b",
   "metadata": {},
   "source": [
    "### 데이터 파일 읽기, 문장 단위로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9323e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 187088\n",
      "examples:\n",
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "txt_file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "# 문장 단위로 저장\n",
    "raw_corpus = []\n",
    "for txt in txt_list:\n",
    "    with open(txt, \"r\") as f: \n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"data size:\", len(raw_corpus))\n",
    "print(\"examples:\")\n",
    "print(raw_corpus[0])\n",
    "print(raw_corpus[1])\n",
    "print(raw_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fc9f3",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedca82",
   "metadata": {},
   "source": [
    "### 작사가를 만들기 위한 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6071c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 토큰화를 위한 문장 정제\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 모두 소문자로\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) # 마침표, 물음표, 느낌표, 쉼표 중복 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence) # 소문자, 대문자, 마침표, 물음표, 느낌표 외 제거\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 다중 공백 제거\n",
    "    sentence = sentence.strip() # 문장 처음과 끝 공백 제거\n",
    "    sentence = \"<start> \" + sentence + \" <end>\" # 문장 처음과 끝 <start> <end> 추가\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7445fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제된 문장 저장\n",
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue # 빈 문장 제외\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence) # 문장 정제 함수 실행\n",
    "    if 15 < len(preprocessed_sentence.split()): continue # 토큰의 개수가 15개 초과하는 경우 제외\n",
    "    \n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba40ab",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2baf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 토큰화\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000, # 단어 개수\n",
    "        filters = ' ',\n",
    "        oov_token = \"<unk>\" # 예외 단어\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # corpus(문장) -> tensor 변환\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding= 'post') # post(뒷 부분 pad) <-> pre(앞 부분 pad) \n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78689fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    <unk>\n",
      "2    <start>\n",
      "3    <end>\n",
      "4    i   \n",
      "5    ,   \n",
      "6    the \n",
      "7    you \n",
      "8    and \n",
      "9    a   \n",
      "10   to  \n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(\"{:<4} {:<4}\".format(idx, tokenizer.index_word[idx]))\n",
    "    if 10 <= idx: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cbcf8e",
   "metadata": {},
   "source": [
    "### 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83b4e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n",
      "[ 2 49  5  3  0  0  0  0  0  0  0  0  0  0]\n",
      "[49  5  3  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src = tensor[:, :-1] # <pad> 제외 (적은 가능성으로 <end>가 제외)\n",
    "tgt = tensor[:, 1:] # <start> 제외\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src, tgt, test_size = 0.2, random_state = 49)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(enc_val[0])\n",
    "print(dec_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be6793",
   "metadata": {},
   "source": [
    "### dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d65510f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 14), (128, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# dataset 객체 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder = True))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2c8cd",
   "metadata": {},
   "source": [
    "## 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fcd72",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cf9413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 레이어\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) # embedding layer (input, vector - id mapping)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True) # LSTM layer\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True) \n",
    "        self.linear = tf.keras.layers.Dense(vocab_size) # dense layer (output)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1024 # word vector\n",
    "hidden_size = 2048 # LSTM layer\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0d957",
   "metadata": {},
   "source": [
    "### 훈련 (val_loss 2.2 이하)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a08751ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7a843e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "976/976 [==============================] - 295s 284ms/step - loss: 3.0759\n",
      "Epoch 2/10\n",
      "976/976 [==============================] - 282s 289ms/step - loss: 2.5273\n",
      "Epoch 3/10\n",
      "976/976 [==============================] - 283s 289ms/step - loss: 2.1239\n",
      "Epoch 4/10\n",
      "976/976 [==============================] - 283s 289ms/step - loss: 1.7544\n",
      "Epoch 5/10\n",
      "976/976 [==============================] - 283s 290ms/step - loss: 1.4517\n",
      "Epoch 6/10\n",
      "976/976 [==============================] - 283s 290ms/step - loss: 1.2340\n",
      "Epoch 7/10\n",
      "976/976 [==============================] - 283s 290ms/step - loss: 1.0994\n",
      "Epoch 8/10\n",
      "976/976 [==============================] - 284s 291ms/step - loss: 1.0309\n",
      "Epoch 9/10\n",
      "976/976 [==============================] - 283s 290ms/step - loss: 1.0001\n",
      "Epoch 10/10\n",
      "976/976 [==============================] - 283s 290ms/step - loss: 0.9836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fd37cc910>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be3a42",
   "metadata": {},
   "source": [
    "### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "096b0745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수\n",
    "def generate_text(model, tokenizer, init_sentence = \"<start>\", max_len = 14):\n",
    "    # init_sentence -> tensor 변환\n",
    "    init_sequence = tokenizer.texts_to_sequences([init_sentence])\n",
    "    result_tensor = tf.convert_to_tensor(init_sequence, dtype = tf.int64)\n",
    "    \n",
    "    while True:\n",
    "        predict = model(result_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis = -1), axis = -1)[:, -1]\n",
    "        result_tensor = tf.concat([result_tensor, tf.expand_dims(predict_word, axis = 0)], axis = -1)\n",
    "        \n",
    "        if predict_word.numpy()[0] == tokenizer.word_index[\"<end>\"]: break\n",
    "        if result_tensor.shape[1] >= max_len: break\n",
    "    \n",
    "    text = \"\"\n",
    "    for word_index in result_tensor[0].numpy():\n",
    "        text += tokenizer.index_word[word_index] + \" \"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdf3c140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you more than i love myself , <end> '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "817eaf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977/977 [==============================] - 39s 40ms/step - loss: 2.2219\n"
     ]
    }
   ],
   "source": [
    "# test data 평가\n",
    "score = model.evaluate(enc_val, dec_val, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ac873",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd810fa1",
   "metadata": {},
   "source": [
    "## 프로젝트 평가 및 한계점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce264b64",
   "metadata": {},
   "source": [
    "- 결과물의 퀄리티<br/>\n",
    "\"i love you more than i love myself ,\"<br/>\n",
    "내가 얻은 문장은 쉼표로 끝나는 오류가 있다.<br/>\n",
    "하지만 쉼표로 끝나는 것 외에는 문법적으로나 의미적으로 어색하지 않은 것 같다.<br/>\n",
    "사랑 노래의 가사로도 나올 법한 문장인 것 같아 이 정도면 훌륭하지 않나 생각한다.<br/>\n",
    "다만 실제로 이 모델을 사용한다고 가정하면 모델이 문장을 생성한 뒤 사람이 직접 평가를 해야할 필요가 있다.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c766cc1",
   "metadata": {},
   "source": [
    "- loss<br/>\n",
    "test data로 평가한 loss가 2.2 수준으로 나와 node의 요구사항에 부합한다.<br/>\n",
    "하지만 model.fit 단계에서 예측한 loss와 test data로 평가한 loss가 약 2.5배 차이나므로 이를 줄일 필요가 있다.<br/>\n",
    "overfitting 방지하는 방법으로는 regularization strength를 올리는 방법이 있다.<br/>\n",
    "시간이 부족하므로 현재는 수행하지 못했다.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c67b510",
   "metadata": {},
   "source": [
    "- train/test 데이터 분리에 대한 의문<br/>\n",
    "node에서는 토큰화를 실행한 뒤, train/test 데이터를 분리하라고 지시되어 있지만, 다음 글에서는 토큰화 후 데이터 분리에 대해 부정적인 의견을 표하고 있다.<br/>\n",
    "아직 지식이 없어 어떤 상황에서, 어떤 이유에서 이러한 조언을 주는 지 파악하지 못해 아쉬움이 남는다.<br/>\n",
    "<br/>\n",
    "[stackoverflow, processing before or after train test split, (2022.01.18)](https://stackoverflow.com/questions/57693333/processing-before-or-after-train-test-split)\n",
    "<br/>\n",
    "[StackAbuse, Python for NLP: Multi-label Text Classification with Keras, Usman Malik, (2022.01.18)](https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95fe7a",
   "metadata": {},
   "source": [
    "## 학습 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f317bd",
   "metadata": {},
   "source": [
    "- with open(...) as f<br/>\n",
    "\"with open A as B\" 이 구문에서 A는 'context manager'가 되는 객체이고, B는 A enter 메서드의 리턴값이다.<br/>\n",
    "즉, 위 구문의 f는 open(...) 함수가 리턴한 file object이고, f = open(...)라고 볼 수 있다.<br/>\n",
    "<br/>\n",
    "[Python)with open(...) as f에서 f의 정체는?, Jun-young Cha, (2020.02.22)](https://starriet.medium.com/python-with-open-as-f-%EC%97%90%EC%84%9C-f%EC%9D%98-%EC%A0%95%EC%B2%B4%EB%8A%94-3cb48ea9e302)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb8bab",
   "metadata": {},
   "source": [
    "- 정규표현식<br/>\n",
    "언어를 원하는 형식으로 맞추기 위해서 정규표현식이 용이하다.<br/>\n",
    "정규표현식은 다양한 활용 방안이 있는데, 이때 이용하기 좋은 사이트가 있다.<br/>\n",
    "<br/>\n",
    "[정규표현식](https://regexr.com/)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c3471",
   "metadata": {},
   "source": [
    "- list append()와 extend()<br/>\n",
    "list.append(iterable)은 리스트 끝에 iterable을 그대로 넣습니다.<br/>\n",
    "list.extend(iterable)은 리스트 끝에 iterable의 모든 항목을 넣습니다.<br/>\n",
    "<br/>\n",
    "[Python- list append()와 extend()의 차이점, 네이버 블로그, apple, (2019.05.19)](https://m.blog.naver.com/wideeyed/221541104629)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5ea3f",
   "metadata": {},
   "source": [
    "- LSTM(Long-Short-Term Memory)<br/>\n",
    "이 모델은 LSTM layer를 사용하는데, LSTM은 RNN의 한 종류이다.<br/>\n",
    "여러 문장의 어떤 키워드를 추측할 때, 뒷 맥락을 모두 알아야 한다면 즉 long-term dependencies를 다뤄야 한다면 기존 RNN 보다 LSTM을 사용하는 편이 좋다.<br/>\n",
    "<br/>\n",
    "[Long-Short-Term Memory (LSTM) 이해하기, Machine Learning, (2018.04.10)](https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr)\n",
    "<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
